import os
import sys
import csv
import pandas as pd
from pyspark.sql.functions import (
    col, count, avg, expr, from_unixtime, date_format,
    split, explode, lower, trim, concat_ws, when # ç¡®ä¿ when è¢«å¯¼å…¥
)
from pyspark.sql.types import TimestampType, StringType, LongType, IntegerType
from analysis.sentiment_trend_predictor import EnhancedSentimentTrendPredictor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.spark_utils import get_spark_session, stop_spark_session
from src.processing.sentiment_analyzer import analyze_sentiment_batch
from src.data_ingestion.simulator import generate_simulated_data
from src.data_ingestion.db_reader import read_table_from_db

def run_sentiment_analysis():
    """è¿è¡Œå®Œæ•´çš„æƒ…æ„Ÿåˆ†ææµç¨‹"""
    
    spark = None
    try:
        # 1. åˆå§‹åŒ–Spark
        print("ğŸš€ åˆå§‹åŒ–Sparkç¯å¢ƒ...")
        spark = get_spark_session("SocialMediaSentimentAnalysis")
        
        # 2. ç”Ÿæˆæˆ–åŠ è½½æ•°æ®
        data_file = "data/raw/simulated_posts.csv"
        if not os.path.exists(data_file):
            print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
            generate_simulated_data(5000, data_file)
        
        # 3. åŠ è½½æ•°æ®
        print("ğŸ“– åŠ è½½æ•°æ®...")
        df = spark.read.csv(data_file, header=True, inferSchema=True)
        
        print(f"æ•°æ®æ€»é‡: {df.count()} æ¡")
        print("æ•°æ®ç»“æ„:")
        df.printSchema()
        print("æ•°æ®æ ·ä¾‹:")
        df.show(5, truncate=False)
        
        # 4. æ•°æ®æ¸…ç†
        print("ğŸ§¹ æ•°æ®æ¸…ç†...")
        df_clean = df.filter(col("text").isNotNull() & (col("text") != ""))

        
        # 5. æƒ…æ„Ÿåˆ†æ
        print("ğŸ­ æ‰§è¡Œæƒ…æ„Ÿåˆ†æ...")
        df_sentiment = analyze_sentiment_batch(df_clean)
        
        # ç¼“å­˜ç»“æœä»¥æé«˜åç»­æ“ä½œæ€§èƒ½
        df_sentiment.cache()
        
        # 6. åŸºç¡€ç»Ÿè®¡
        print("\nğŸ“ˆ æƒ…æ„Ÿåˆ†æç»“æœç»Ÿè®¡:")
        sentiment_stats = df_sentiment.groupBy("sentiment_category").agg(
            count("*").alias("count"),
            avg("sentiment_score").alias("avg_score")
        ).orderBy("count", ascending=False)
        
        sentiment_stats.show()
        
        # 7. å¹³å°ç»Ÿè®¡
        print("\nğŸ“± å„å¹³å°æƒ…æ„Ÿåˆ†å¸ƒ:")
        platform_sentiment = df_sentiment.groupBy("platform", "sentiment_category").count().orderBy("platform", "count")
        platform_sentiment.show()
        
        # 8. ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "data/processed"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœä¸ºParquetæ ¼å¼
        df_sentiment.write.mode("overwrite").parquet(f"{output_dir}/sentiment_results.parquet")
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœä¸ºCSV
        sentiment_stats_pd = sentiment_stats.toPandas()
        sentiment_stats_pd.to_csv(
            f"{output_dir}/sentiment_distribution.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )
        
        platform_sentiment_pd = platform_sentiment.toPandas()
        platform_sentiment_pd.to_csv(
            f"{output_dir}/platform_sentiment.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )
        
        # ä¿å­˜è¯¦ç»†ç»“æœçš„æ ·æœ¬ä¸ºCSVï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
        sample_results = df_sentiment.select("text", "sentiment_score", "sentiment_category", "platform").limit(1000)
        sample_results_pd = sample_results.toPandas()
        sample_results_pd.to_csv(
            f"{output_dir}/sentiment_sample.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )
        
        print("âœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° data/processed/ ç›®å½•")
        print(f"- å®Œæ•´ç»“æœ: {output_dir}/sentiment_results.parquet")
        print(f"- æƒ…æ„Ÿåˆ†å¸ƒ: {output_dir}/sentiment_distribution.csv")
        print(f"- å¹³å°åˆ†æ: {output_dir}/platform_sentiment.csv")
        print(f"- æ ·æœ¬æ•°æ®: {output_dir}/sentiment_sample.csv")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback # ç¡®ä¿ traceback è¢«å¯¼å…¥
        traceback.print_exc()
        return False
    
    finally:
        if spark:
            stop_spark_session(spark)


def run_offline_sql_analysis():
    """
    å¯¹æ¥è‡ª MySQL çš„ç¬”è®°(title + desc)å’Œè¯„è®º(content)ä¸€èµ·è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œ
    å¹¶æŒ‰ IP å±åœ°ã€æ—¶é—´ç­‰ç»´åº¦ç»Ÿè®¡ï¼›åŒæ—¶å¯¹ç¬”è®°æ ‡ç­¾è¿›è¡Œæ‹†åˆ†ç»Ÿè®¡ï¼Œä¾›å‰ç«¯è¯äº‘ä½¿ç”¨ã€‚
    æœ€åè¿›è¡Œæƒ…æ„Ÿè¶‹åŠ¿é¢„æµ‹ã€‚
    """
    spark = None
    try:
        print("ğŸš€ åˆå§‹åŒ– Spark ç¯å¢ƒ (æ•°æ®åº“ SQL åˆ†æ)...")
        spark = get_spark_session("SocialMediaSentimentAnalysis_DBSQL")

        # ----------------------------
        # 1. ä» MySQL è¯»å–ç¬”è®°è¡¨å’Œè¯„è®ºè¡¨
        # ----------------------------
        print("ğŸ“– æ­£åœ¨ä» MySQL æ•°æ®åº“è¯»å– xhs_note è¡¨å’Œ xhs_note_comment è¡¨...")
        df_notes_raw = read_table_from_db(spark, "xhs_note") # é‡å‘½åä¸º _raw ä»¥ç¤ºåŒºåˆ†
        df_comments_raw = read_table_from_db(spark, "xhs_note_comment") # é‡å‘½åä¸º _raw

        output_db_dir = "data/processed/db_analysis"
        print(f"ğŸ“ ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨: {output_db_dir}")
        os.makedirs(output_db_dir, exist_ok=True)
        # -----------------------------------------------------------------
        # 2. æ•°æ®ç±»å‹è½¬æ¢å’Œåˆæ­¥æ¸…ç† (åœ¨è¿‡æ»¤ä¹‹å‰è¿›è¡Œï¼Œç¡®ä¿è¿‡æ»¤æ¡ä»¶åŸºäºæ­£ç¡®ç±»å‹)
        # -----------------------------------------------------------------
        print("âš™ï¸ è¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢å’Œåˆæ­¥æ¸…ç†...")

        # --- å¤„ç† df_notes ---
        df_notes = df_notes_raw \
            .withColumn( # å¤„ç† liked_count
                "liked_count_cleaned",
                when(trim(col("liked_count")) == "", "0") # ç©ºå­—ç¬¦ä¸²è§†ä¸º "0"
                .when(col("liked_count").isNull(), "0")   # NULL è§†ä¸º "0"
                .otherwise(col("liked_count"))
            ) \
            .withColumn("liked_count", col("liked_count_cleaned").cast(LongType())) \
            .drop("liked_count_cleaned") \
            .withColumn( # å¤„ç† comment_count
                "comment_count_cleaned",
                when(trim(col("comment_count")) == "", "0") # ç©ºå­—ç¬¦ä¸²è§†ä¸º "0"
                .when(col("comment_count").isNull(), "0")   # NULL è§†ä¸º "0"
                .otherwise(col("comment_count"))
            ) \
            .withColumn("comment_count", col("comment_count_cleaned").cast(LongType())) \
            .drop("comment_count_cleaned")


        print("ç¬”è®°è¡¨æ•°æ®ç»“æ„:")
        df_notes.printSchema()

        df_comments = df_comments_raw

        # ----------------------------
        # 2.b æ•°æ®è´¨é‡æ£€æŸ¥ä¸è¿‡æ»¤ (ç°åœ¨åŸºäºå·²è½¬æ¢å’Œåˆæ­¥æ¸…ç†çš„æ•°æ®)
        # ----------------------------
        print("ğŸ§¹ æ•°æ®è´¨é‡æ£€æŸ¥ä¸è¿‡æ»¤...")
        # ç¬”è®°ï¼šè¦æ±‚ note_id éç©ºï¼Œtitle æˆ– desc è‡³å°‘æœ‰ä¸€ä¸ªä¸ä¸ºç©º
        df_notes = df_notes.filter(
            col("note_id").isNotNull() &
            (
                (col("title").isNotNull() & (trim(col("title")) != "")) | # ä½¿ç”¨ trim é¿å…çº¯ç©ºæ ¼
                (col("desc").isNotNull() & (trim(col("desc")) != ""))   # ä½¿ç”¨ trim
            ) &
            col("time").isNotNull()  # ç¡®ä¿å‘å¸ƒæ—¶é—´æˆ³å­˜åœ¨
        )

        # è¯„è®ºï¼šè¦æ±‚ note_idã€comment_idã€contentã€create_time å‡ä¸ä¸ºç©º
        df_comments = df_comments.filter(
            col("note_id").isNotNull() &
            col("comment_id").isNotNull() &
            (col("content").isNotNull() & (trim(col("content")) != "")) & # ä½¿ç”¨ trim
            col("create_time").isNotNull()
        )

        notes_count = df_notes.count()
        comments_count = df_comments.count()
        print(f"æœ‰æ•ˆç¬”è®°æ•°æ®: {notes_count} æ¡, æœ‰æ•ˆè¯„è®ºæ•°æ®: {comments_count} æ¡ã€‚")
        if notes_count == 0 and comments_count == 0:
            print("â—â—è­¦å‘Š: ç¬”è®°å’Œè¯„è®ºæ•°æ®éƒ½ä¸ºç©ºï¼Œé€€å‡ºåˆ†æã€‚")
            return False

        # ----------------------------
        # 3. ç»Ÿè®¡æ¯ç¯‡ç¬”è®°çš„ç‚¹èµæ•°ä¸è¯„è®ºæ•°
        # ----------------------------
        df_notes.createOrReplaceTempView("notes") # notes è§†å›¾ç°åœ¨åŒ…å«è½¬æ¢åçš„æ•°å€¼åˆ—
        df_comments.createOrReplaceTempView("comments")

        note_stats_query = """
            SELECT
                n.note_id,
                n.title,
                COALESCE(n.liked_count, 0) AS liked_count, 
                COALESCE(n.comment_count, 0) AS note_declared_comment_count,
                COUNT(c.comment_id) AS actual_comment_count
            FROM notes n
            LEFT JOIN comments c ON n.note_id = c.note_id
            GROUP BY n.note_id, n.title, n.liked_count, n.comment_count
            ORDER BY actual_comment_count DESC
        """
        df_note_stats = spark.sql(note_stats_query)
        print("\nğŸ“Š æ¯ç¯‡ç¬”è®°ç»Ÿè®¡ä¿¡æ¯ (ç¬”è®°è¡¨å£°æ˜è¯„è®ºæ•° vs å®é™…è¯„è®ºè¡¨ç»Ÿè®¡):")
        df_note_stats.show(10, truncate=False)

        # ----------------------------
        # 4. è¯„è®ºæŒ‰å¤©åˆ†å¸ƒç»Ÿè®¡
        # ----------------------------
        df_comments_with_date = df_comments.withColumn(
            "comment_date",
            date_format(from_unixtime((col("create_time") / 1000).cast("long")), "yyyy-MM-dd")
        )
        df_comment_daily_dist = (
            df_comments_with_date
            .groupBy("comment_date")
            .count()
            .withColumnRenamed("count", "num_comments")
            .orderBy("comment_date")
        )
        print("\nğŸ—“ï¸ è¯„è®ºæŒ‰å¤©åˆ†å¸ƒç»Ÿè®¡:")
        df_comment_daily_dist.show(10)

        # ----------------------------
        # 5. åˆå¹¶ç¬”è®°å’Œè¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        # ----------------------------
        # 5.1 å¤„ç†è¯„è®ºï¼šé‡å‘½å content -> textï¼Œä¿ç•™ create_timeã€ip_locationã€note_id
        df_comments_for_sentiment = (
            df_comments
            .select(
                col("comment_id").alias("id"),
                col("note_id"),
                col("content").alias("text"),
                (col("create_time") / 1000).cast("long").alias("create_time"), 
                col("ip_location")
            )
        )
        # 5.2 å¤„ç†ç¬”è®°ï¼šå°† title + desc æ‹¼æ¥ä¸º textï¼Œä¿ç•™ time ä½œä¸º create_timeï¼Œip_locationã€note_id
        df_notes_for_sentiment = (
            df_notes
            .withColumn(
                "text",
                concat_ws(" ", col("title"), col("desc")) # title/desc å¯èƒ½ä¸º NULLï¼Œconcat_ws ä¼šå¤„ç†
            )
            .select(
                col("note_id").alias("id"), # ä½œä¸ºå”¯ä¸€æ ‡è¯†
                col("note_id").alias("source_note_id"), # ä¿ç•™åŸå§‹ç¬”è®°IDç”¨äºå…³è”æˆ–è¿½æº¯
                col("text"),
                (col("time") / 1000).cast("long").alias("create_time"), 
                col("ip_location")
            )
        )

        df_notes_for_sentiment = df_notes_for_sentiment.filter(trim(col("text")) != "")
        
        # ç¡®ä¿åˆ—åå’Œç±»å‹ä¸€è‡´ä»¥ä¾¿ unionByName
        # df_comments_for_sentiment å·²ç»æœ‰ id, note_id, text, create_time, ip_location
        # df_notes_for_sentiment æœ‰ id (æ¥è‡ªnote_id), source_note_id, text, create_time, ip_location
        # ä¸ºäº† unionByName, æˆ‘ä»¬éœ€è¦ç»Ÿä¸€åˆ—ã€‚
        # æ–¹æ¡ˆï¼šç»™ç¬”è®°çš„ id ä¹Ÿç”¨ note_id, ç„¶åæ·»åŠ ä¸€ä¸ª type åˆ—åŒºåˆ†æ˜¯ç¬”è®°è¿˜æ˜¯è¯„è®ºã€‚
        # æˆ–è€…ï¼Œå¦‚æœä¸éœ€è¦åŒºåˆ†ï¼Œä¸”æƒ…æ„Ÿåˆ†æçš„ id åªæ˜¯ä¸€ä¸ªå”¯ä¸€é”®ï¼Œå½“å‰ note_id as id ä¹Ÿå¯ä»¥ã€‚
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾æƒ…æ„Ÿåˆ†æåªéœ€è¦ä¸€ä¸ªå”¯ä¸€ id, text, create_time, ip_location
        # ç¬”è®°çš„ note_id å­—æ®µåœ¨ df_notes_for_sentiment ä¸­å·²ç»é€šè¿‡ alias("id") æ˜ å°„ä¸º id
        # è¯„è®ºçš„ note_id å­—æ®µåœ¨ df_comments_for_sentiment ä¸­æ˜¯ note_id.
        # ä¸ºäº†ç»Ÿä¸€ï¼Œæˆ‘ä»¬ç¡®ä¿ä¸¤ä¸ª DataFrame éƒ½æœ‰ `note_id` å­—æ®µä¾›æƒ…æ„Ÿåˆ†æåå¯èƒ½çš„å›æº¯ï¼Œ
        # ä»¥åŠä¸€ä¸ªç»Ÿä¸€çš„ `id` å­—æ®µã€‚

        df_comments_for_sentiment = df_comments_for_sentiment.withColumn("type", expr("'comment'"))
        df_notes_for_sentiment = df_notes_for_sentiment.withColumn("type", expr("'note'")) \
                                                       .withColumn("note_id", col("source_note_id")) \
                                                       .drop("source_note_id")


        print("Schema for comments before union:")
        df_comments_for_sentiment.printSchema()
        print("Schema for notes before union:")
        df_notes_for_sentiment.printSchema()


        # df_combined_texts = df_comments_for_sentiment.unionByName(df_notes_for_sentiment, allowMissingColumns=True)
        # ç”±äºåˆ—åå·²å°½é‡å¯¹é½ï¼Œå¯ä»¥ç›´æ¥ unionByName
        # ç¡®ä¿é€‰å–çš„åˆ—æ˜¯å…±æœ‰çš„ï¼Œæˆ–è€…ä½¿ç”¨ allowMissingColumns=True (Spark 3.0+)
        # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªé€‰å–æ ¸å¿ƒåˆ—è¿›è¡Œæƒ…æ„Ÿåˆ†æ
        common_columns = ["id", "text", "create_time", "ip_location", "note_id", "type"]
        df_combined_texts = df_comments_for_sentiment.select(common_columns).unionByName(df_notes_for_sentiment.select(common_columns))


        print(f"\nğŸ­ å¯¹è¯„è®ºå’Œç¬”è®°ï¼ˆå…± {df_combined_texts.count()} æ¡ï¼‰ä¸€èµ·æ‰§è¡Œæƒ…æ„Ÿåˆ†æ...")
        if df_combined_texts.count() == 0:
            print("â—â—è­¦å‘Š: åˆå¹¶åç”¨äºæƒ…æ„Ÿåˆ†æçš„æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æƒ…æ„Ÿåˆ†æåŠåç»­æ­¥éª¤ã€‚")
        else:
            df_combined_sentiment = analyze_sentiment_batch(df_combined_texts)
            df_combined_sentiment.cache()
            print("åˆå¹¶åæƒ…æ„Ÿåˆ†æç»“æœç¤ºä¾‹ (æ˜¾ç¤º note_id å’Œ type):")
            df_combined_sentiment.select("id", "note_id", "type", "text", "sentiment_score", "sentiment_category").show(5, truncate=False)

            # 5.3 åˆå¹¶åçš„æ•´ä½“æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡
            print("\nğŸ“ˆ åˆå¹¶åï¼ˆç¬”è®° + è¯„è®ºï¼‰æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡:")
            combined_sentiment_summary = (
                df_combined_sentiment
                .groupBy("sentiment_category")
                .agg(
                    count("*").alias("count"),
                    avg("sentiment_score").alias("avg_score")
                )
                .orderBy(col("count").desc()) # æ˜ç¡®æŒ‡å®š col
            )
            combined_sentiment_summary.show()

            # 5.4 åˆå¹¶å æƒ…æ„Ÿéšæ—¶é—´å˜åŒ–è¶‹åŠ¿
            # create_time å·²ç»æ˜¯ç§’çº§æ—¶é—´æˆ³
            df_combined_sentiment_dated = df_combined_sentiment.withColumn(
                "date",
                date_format(from_unixtime(col("create_time")), "yyyy-MM-dd")
            )
            sentiment_over_time = (
                df_combined_sentiment_dated
                .groupBy("date", "sentiment_category")
                .count()
                .orderBy("date", "sentiment_category")
            )
            print("\nğŸ“‰ åˆå¹¶åæƒ…æ„Ÿéšæ—¶é—´å˜åŒ–è¶‹åŠ¿:")
            sentiment_over_time.show(10)

            # 5.5 åˆå¹¶å æŒ‰ IP å±åœ°ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
            sentiment_by_location = (
                df_combined_sentiment
                .filter(col("ip_location").isNotNull() & (trim(col("ip_location")) != "")) # ä½¿ç”¨ trim
                .groupBy("ip_location", "sentiment_category")
                .count()
                .orderBy(col("count").desc())
            )
            print("\nğŸ—ºï¸ åˆå¹¶åè¯„è®º+ç¬”è®°æƒ…æ„ŸæŒ‰ IP å±åœ°åˆ†å¸ƒ (Top 10):")
            sentiment_by_location.show(10, truncate=False)
            
            # --- ä¿å­˜æƒ…æ„Ÿåˆ†æç›¸å…³ç»“æœ ---
            output_db_dir = "data/processed/db_analysis" 
            
            combined_sentiment_summary.toPandas().to_csv(
                f"{output_db_dir}/combined_sentiment_distribution.csv",
                index=False,
                quoting=csv.QUOTE_NONNUMERIC,
                encoding='utf-8-sig'
            )
            sentiment_over_time.toPandas().to_csv(
                f"{output_db_dir}/combined_sentiment_over_time.csv",
                index=False,
                quoting=csv.QUOTE_NONNUMERIC,
                encoding='utf-8-sig'
            )
            sentiment_by_location.limit(200).toPandas().to_csv(
                f"{output_db_dir}/combined_sentiment_by_location_sample.csv",
                index=False,
                quoting=csv.QUOTE_NONNUMERIC,
                encoding='utf-8-sig'
            )
            # å¯ä»¥è€ƒè™‘ä¿å­˜ df_combined_sentiment çš„æ ·æœ¬æˆ–å…¨é‡ (Parquet)
            df_combined_sentiment.select("id", "note_id", "type", "text", "sentiment_score", "sentiment_category", "create_time", "ip_location") \
                .limit(5000) \
                .toPandas() \
                .to_csv(
                    f"{output_db_dir}/combined_sentiment_sample_details.csv",
                    index=False,
                    quoting=csv.QUOTE_NONNUMERIC,
                    encoding='utf-8-sig',
                    escapechar='\\'  # æ·»åŠ è½¬ä¹‰å­—ç¬¦
                )

            # ----------------------------
            # 5.6 æƒ…æ„Ÿè¶‹åŠ¿é¢„æµ‹
            # ----------------------------
            print("\nğŸ”® å¼€å§‹æƒ…æ„Ÿè¶‹åŠ¿é¢„æµ‹...")
            predictor = EnhancedSentimentTrendPredictor(spark)
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            prediction_data = df_combined_sentiment.select(
                "sentiment_score",
                "create_time"
            )
            
            # è¿è¡Œé¢„æµ‹æµç¨‹
            prediction_results = predictor.run_prediction_pipeline(
                prediction_data,
                test_size=0.2,
                days_to_predict=30
            )
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            future_predictions = prediction_results["future_predictions"]
            future_predictions.toPandas().to_csv(
                f"{output_db_dir}/sentiment_trend_predictions.csv",
                index=False,
                quoting=csv.QUOTE_NONNUMERIC,
                encoding='utf-8-sig'
            )
            
            # ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
            evaluation_metrics = prediction_results["evaluation"]
            pd.DataFrame([evaluation_metrics]).to_csv(
                f"{output_db_dir}/sentiment_trend_evaluation.csv",
                index=False,
                quoting=csv.QUOTE_NONNUMERIC,
                encoding='utf-8-sig'
            )
            
            print("\nğŸ“ˆ æƒ…æ„Ÿè¶‹åŠ¿é¢„æµ‹ç»“æœ:")
            future_predictions.show(10)
            print("\nğŸ“Š æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
            print(f"- RMSE: {evaluation_metrics['rmse']:.4f}")

        # ----------------------------
        # 6. çƒ­é—¨æ ‡ç­¾åˆ†æï¼ˆtag_list ä»¥é€—å·åˆ†éš”ï¼‰
        # ----------------------------
        print("\nğŸ”¥ çƒ­é—¨æ ‡ç­¾åˆ†æ (ä»ç¬”è®° tag_list æ‹†åˆ†):")
        df_tags = (
            df_notes # ä½¿ç”¨å·²ç»è¿‡ç±»å‹è½¬æ¢å’Œåˆæ­¥è¿‡æ»¤çš„ df_notes
            .select("note_id", "tag_list")
            .filter(col("tag_list").isNotNull() & (trim(col("tag_list")) != "")) # ä½¿ç”¨ trim
            .withColumn("tag", explode(split(trim(col("tag_list")), ","))) # å¯¹ tag_list ä¹Ÿ trim ä¸€ä¸‹
            .select(
                lower(trim(col("tag"))).alias("tag") # å¯¹å•ä¸ª tag ä¹Ÿ trim
            )
            .filter(trim(col("tag")) != "") # ç¡®ä¿ tag éç©º
            .groupBy("tag")
            .agg(count("*").alias("tag_count"))
            .orderBy(col("tag_count").desc())
        )
        df_tags.show(10, truncate=False)

        # ----------------------------
        # 7. ä¿å­˜æ‰€æœ‰åˆ†æç»“æœåˆ°æœ¬åœ°
        # ----------------------------
        output_db_dir = "data/processed/db_analysis" # ç›®å½•å˜é‡å¤ç”¨
        os.makedirs(output_db_dir, exist_ok=True) # å†æ¬¡ç¡®ä¿ï¼Œè™½ç„¶å‰é¢å¯èƒ½æœ‰

        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åº“åˆ†æç»“æœåˆ° {output_db_dir}/ ...")
        # 7.1 ä¿å­˜ç¬”è®°ç‚¹èµ/è¯„è®ºç»Ÿè®¡
        df_note_stats.toPandas().to_csv(
            f"{output_db_dir}/note_comment_stats.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )
        # 7.2 ä¿å­˜è¯„è®ºæŒ‰å¤©åˆ†å¸ƒ
        df_comment_daily_dist.toPandas().to_csv(
            f"{output_db_dir}/comment_daily_distribution.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )
        # 7.6 ä¿å­˜çƒ­é—¨æ ‡ç­¾ï¼Œç”¨äºå‰ç«¯è¯äº‘
        df_tags.limit(200).toPandas().to_csv(
            f"{output_db_dir}/top_tags_for_wordcloud.csv",
            index=False,
            quoting=csv.QUOTE_NONNUMERIC,
            encoding='utf-8-sig'
        )

        print(f"âœ… æ•°æ®åº“ SQL åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ {output_db_dir}/ ç›®å½•")
        return True

    except Exception as e:
        print(f"âŒ æ•°æ®åº“ SQL åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if spark:
            stop_spark_session(spark)

if __name__ == "__main__":
    print("=== Step 1: è¿è¡Œæ¨¡æ‹Ÿæ•°æ®æƒ…æ„Ÿåˆ†ææµç¨‹ ===")
    run_first = True # æ§åˆ¶æ˜¯å¦è¿è¡Œæ¨¡æ‹Ÿæ•°æ®åˆ†æï¼Œæ–¹ä¾¿è°ƒè¯•
    if run_first:
        success1 = run_sentiment_analysis()
        if not success1:
            print("æ¨¡æ‹Ÿæ•°æ®åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ã€‚")
    else:
        success1 = True # å‡è®¾æˆåŠŸä»¥ä¾¿ç»§ç»­
        print("è·³è¿‡æ¨¡æ‹Ÿæ•°æ®åˆ†ææµç¨‹ã€‚")


    print("\n=== Step 2: è¿è¡Œæ•°æ®åº“ç¦»çº¿ SQL åˆ†ææµç¨‹ ===")
    success2 = run_offline_sql_analysis()

    if success1 and success2: # ç¡®ä¿ä¸¤ä¸ªéƒ½å®šä¹‰äº†
        print("\nğŸ‰ æ‰€æœ‰é€‰å®šæµç¨‹æ‰§è¡ŒæˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡Œå‰ç«¯æŸ¥çœ‹ç»“æœï¼š")
        print("ğŸ‘‰ streamlit run frontend/app.py")
    else:
        print("\nâš ï¸ æœ‰éƒ¨åˆ†æµç¨‹æ‰§è¡Œå¤±è´¥æˆ–æœªæ‰§è¡Œï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯ã€‚")