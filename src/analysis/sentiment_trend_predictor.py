from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, avg, count, when, lit, expr, date_format, from_unixtime,
    lag, mean, stddev, dayofweek, month, year
)
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.types import DoubleType, DateType
from datetime import datetime, timedelta
import numpy as np
import pandas as pd

class EnhancedSentimentTrendPredictor:
    
    def __init__(self, spark):
        self.spark = spark
        self.trained_model = None
        self.feature_columns = []
        
    def prepare_features(self, df):
        print("ğŸ“Š å‡†å¤‡ç‰¹å¾...")
        
        # æ•°æ®æ¸…ç†
        df = df.filter(
            col("sentiment_score").isNotNull() & 
            col("create_time").isNotNull()
        )
        
        # è½¬æ¢æ—¶é—´æ ¼å¼å¹¶èšåˆ
        df = df.withColumn("date", date_format(from_unixtime(col("create_time")), "yyyy-MM-dd"))
        
        # æŒ‰æ—¥æœŸèšåˆ
        daily_sentiment = df.groupBy("date") \
            .agg(
                avg("sentiment_score").alias("avg_sentiment"),
                count("*").alias("post_count"),
                stddev("sentiment_score").alias("sentiment_std")
            ) \
            .orderBy("date")
        
        # åˆ›å»ºæ—¶é—´ç‰¹å¾
        window_spec = Window.orderBy("date")
        df_features = daily_sentiment \
            .withColumn("sentiment_lag_1", lag("avg_sentiment", 1).over(window_spec)) \
            .withColumn("sentiment_lag_2", lag("avg_sentiment", 2).over(window_spec)) \
            .withColumn("sentiment_ma_7", mean("avg_sentiment").over(window_spec.rowsBetween(-6, 0))) \
            .withColumn("date_parsed", col("date").cast(DateType())) \
            .withColumn("day_of_week", dayofweek(col("date_parsed"))) \
            .withColumn("month", month(col("date_parsed"))) \
            .withColumn("year", year(col("date_parsed")))
        
        # å¤„ç†ç¼ºå¤±å€¼
        df_features = df_features.fillna(0.0)
        
        print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œå…± {df_features.count()} å¤©çš„æ•°æ®")
        return df_features
    
    def prepare_training_data(self, df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“‹ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        exclude_cols = ['date', 'date_parsed', 'avg_sentiment']
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        
        # åˆ›å»ºç‰¹å¾å‘é‡
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features_raw",
            handleInvalid="skip"
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler(
            inputCol="features_raw",
            outputCol="features",
            withStd=True,
            withMean=True
        )
        
        # å‡†å¤‡æ ‡ç­¾
        df_features = assembler.transform(df)
        df_features = df_features.withColumn("label", col("avg_sentiment").cast(DoubleType()))
        
        # åº”ç”¨æ ‡å‡†åŒ–
        scaler_model = scaler.fit(df_features)
        df_scaled = scaler_model.transform(df_features)
        
        self.feature_columns = feature_cols
        self.scaler_model = scaler_model
        
        return df_scaled
    
    def train_model(self, training_data):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸ¤– è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        model = RandomForestRegressor(
            featuresCol="features",
            labelCol="label",
            numTrees=50,
            maxDepth=10
        )
        
        pipeline = Pipeline(stages=[model])
        trained_model = pipeline.fit(training_data)
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return trained_model
    
    def predict_future(self, model, last_data, days_to_predict=30):
        """é¢„æµ‹æœªæ¥è¶‹åŠ¿"""
        print(f"ğŸ”® é¢„æµ‹æœªæ¥{days_to_predict}å¤©...")
        
        predictions = []
        current_data = last_data.collect()[0].asDict()
        
        for i in range(days_to_predict):
            # è®¡ç®—é¢„æµ‹æ—¥æœŸ
            last_date = datetime.strptime(current_data['date'], '%Y-%m-%d')
            pred_date = last_date + timedelta(days=i+1)
            
            # æ›´æ–°æ—¶é—´ç‰¹å¾
            current_data['date'] = pred_date.strftime('%Y-%m-%d')
            current_data['day_of_week'] = pred_date.weekday() + 1
            current_data['month'] = pred_date.month
            current_data['year'] = pred_date.year
            
            # åˆ›å»ºé¢„æµ‹æ•°æ®æ¡†
            pred_row = self.spark.createDataFrame([current_data])
            
            # é‡æ–°è£…é…ç‰¹å¾
            assembler = VectorAssembler(
                inputCols=self.feature_columns,
                outputCol="features_raw",
                handleInvalid="skip"
            )
            pred_features = assembler.transform(pred_row)
            pred_scaled = self.scaler_model.transform(pred_features)
            
            # è¿›è¡Œé¢„æµ‹
            prediction = model.transform(pred_scaled)
            pred_value = prediction.select("prediction").collect()[0]["prediction"]
            
            # å­˜å‚¨é¢„æµ‹ç»“æœ
            predictions.append({
                'date': current_data['date'],
                'predicted_sentiment': float(pred_value)
            })
            
            # æ›´æ–°æ»åç‰¹å¾
            current_data['sentiment_lag_2'] = current_data['sentiment_lag_1']
            current_data['sentiment_lag_1'] = pred_value
            current_data['avg_sentiment'] = pred_value
        
        # è½¬æ¢ä¸ºDataFrame
        predictions_df = self.spark.createDataFrame(predictions)
        print("âœ… é¢„æµ‹å®Œæˆ")
        
        return predictions_df
    
    def run_prediction_pipeline(self, sentiment_df, test_size=0.2, days_to_predict=30):
        """è¿è¡Œé¢„æµ‹æµç¨‹"""
        print("ğŸš€ å¼€å§‹é¢„æµ‹æµç¨‹...")
        
        try:
            # 1. å‡†å¤‡ç‰¹å¾
            feature_df = self.prepare_features(sentiment_df)
            
            # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
            training_data = self.prepare_training_data(feature_df)
            
            # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            total_rows = training_data.count()
            train_size = int(total_rows * (1 - test_size))
            train_data = training_data.limit(train_size)
            test_data = training_data.subtract(train_data)
            
            # 4. è®­ç»ƒæ¨¡å‹
            model = self.train_model(train_data)
            self.trained_model = model
            
            # 5. è¯„ä¼°æ¨¡å‹
            evaluator = RegressionEvaluator(
                labelCol="label",
                predictionCol="prediction",
                metricName="rmse"
            )
            rmse = evaluator.evaluate(model.transform(test_data))
            print(f"ğŸ“Š æ¨¡å‹RMSE: {rmse:.4f}")
            
            # 6. è·å–æœ€åä¸€å¤©çš„æ•°æ®ç”¨äºé¢„æµ‹
            last_data = feature_df.orderBy(col("date").desc()).limit(1)
            
            # 7. é¢„æµ‹æœªæ¥è¶‹åŠ¿
            future_predictions = self.predict_future(model, last_data, days_to_predict)
            
            print("ğŸ‰ é¢„æµ‹æµç¨‹å®Œæˆ!")
            
            return {
                "model": model,
                "evaluation": {"rmse": rmse},
                "future_predictions": future_predictions
            }
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹æµç¨‹å‡ºé”™: {str(e)}")
            raise e